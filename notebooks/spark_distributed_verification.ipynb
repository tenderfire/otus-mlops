{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82db34c5",
   "metadata": {},
   "source": [
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è Spark\n",
    "\n",
    "–≠—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–æ–≥–æ, —á—Ç–æ Spark –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ.\n",
    "\n",
    "## –ß—Ç–æ –º—ã –ø—Ä–æ–≤–µ—Ä–∏–º:\n",
    "1. **–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–ª–∞—Å—Ç–µ—Ä–µ** - executors, cores, –ø–∞–º—è—Ç—å\n",
    "2. **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä—Ç–∏—Ü–∏–π** - –∫–ª—é—á–µ–≤–æ–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏\n",
    "3. **Spark Web UI** - –≤–∏–∑—É–∞–ª—å–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥\n",
    "4. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á** - –ª–æ–≥–∏ –∏ –º–µ—Ç—Ä–∏–∫–∏\n",
    "5. **–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö** - –∞–Ω–∞–ª–∏–∑ –ø–∞—Ä—Ç–∏—Ü–∏–π –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a25392",
   "metadata": {},
   "source": [
    "## 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Spark\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é, —á—Ç–æ –∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –Ω–æ—É—Ç–±—É–∫–µ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d277b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark successfully initialized\n",
      "Spark version: 3.0.3\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create Spark session\n",
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .appName(\"OTUS_SparkDistributionCheck\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "        .config(\"spark.executor.memory\", \"2g\")\n",
    "        .config(\"spark.driver.memory\", \"1g\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark successfully initialized\")\n",
    "print(f\"Spark version: {spark.version}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de102805",
   "metadata": {},
   "source": [
    "## 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ Spark –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏ –∫–ª–∞—Å—Ç–µ—Ä–µ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572380d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SPARK CLUSTER INFORMATION\n",
      "================================================================================\n",
      "\n",
      "1. Spark Application ID: application_1762054626128_0007\n",
      "2. Spark App Name: OTUS_SparkDistributionCheck\n",
      "3. Spark Master: yarn\n",
      "4. Spark Version: 3.0.3\n",
      "\n",
      "5. Executor Memory: 2g\n",
      "6. Driver Memory: 1g\n",
      "7. Dynamic Allocation: true\n",
      "8. Default Parallelism: Not set\n",
      "\n",
      "9. Executors info: 'StatusTracker' object has no attribute 'getExecutorInfos'\n",
      "   (Spark UI may not be accessible in this environment)\n",
      "\n",
      "================================================================================\n",
      "üí° Spark Web UI –¥–æ—Å—Ç—É–ø–Ω–∞ –ø–æ –∞–¥—Ä–µ—Å—É:\n",
      "   http://<driver-host>:4040\n",
      "   Application ID: application_1762054626128_0007\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# –ü–†–û–í–ï–†–ö–ê 1: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ Spark –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏ –∫–ª–∞—Å—Ç–µ—Ä–µ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SPARK CLUSTER INFORMATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ Spark\n",
    "spark_context = spark.sparkContext\n",
    "print(f\"\\n1. Spark Application ID: {spark_context.applicationId}\")\n",
    "print(f\"2. Spark App Name: {spark_context.appName}\")\n",
    "print(f\"3. Spark Master: {spark_context.master}\")\n",
    "print(f\"4. Spark Version: {spark_context.version}\")\n",
    "\n",
    "# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "conf = spark_context.getConf()\n",
    "print(f\"\\n5. Executor Memory: {conf.get('spark.executor.memory', 'Not set')}\")\n",
    "print(f\"6. Driver Memory: {conf.get('spark.driver.memory', 'Not set')}\")\n",
    "print(f\"7. Dynamic Allocation: {conf.get('spark.dynamicAllocation.enabled', 'Not set')}\")\n",
    "print(f\"8. Default Parallelism: {conf.get('spark.default.parallelism', 'Not set')}\")\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ executors\n",
    "try:\n",
    "    executors_info = spark_context.statusTracker().getExecutorInfos()\n",
    "    print(f\"\\n9. Number of Active Executors: {len(executors_info)}\")\n",
    "    \n",
    "    if len(executors_info) > 0:\n",
    "        print(\"\\n   Executor Details:\")\n",
    "        for i, exec_info in enumerate(executors_info, 1):\n",
    "            print(f\"   Executor {i}:\")\n",
    "            print(f\"      ID: {exec_info.executorId}\")\n",
    "            print(f\"      Host: {exec_info.executorHost}\")\n",
    "            print(f\"      Total Cores: {exec_info.totalCores}\")\n",
    "            print(f\"      Max Tasks: {exec_info.maxTasks}\")\n",
    "        print(\"\\n   ‚úÖ –ù–ê–ô–î–ï–ù–û EXECUTOR'–û–í: Spark —Ä–∞–±–æ—Ç–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ!\")\n",
    "    else:\n",
    "        print(\"\\n   ‚ö†Ô∏è  Executors –µ—â–µ –Ω–µ –∑–∞–ø—É—â–µ–Ω—ã (–≤–æ–∑–º–æ–∂–Ω–æ, –æ–Ω–∏ –∑–∞–ø—É—Å—Ç—è—Ç—Å—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–π –∑–∞–¥–∞—á–µ)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n9. Executors info: {str(e)}\")\n",
    "    print(\"   (Spark UI may not be accessible in this environment)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üí° Spark Web UI –¥–æ—Å—Ç—É–ø–Ω–∞ –ø–æ –∞–¥—Ä–µ—Å—É:\")\n",
    "print(f\"   http://<driver-host>:4040\")\n",
    "print(f\"   Application ID: {spark_context.applicationId}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbec616",
   "metadata": {},
   "source": [
    "## 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä—Ç–∏—Ü–∏–π (–∫–ª—é—á–µ–≤–æ–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏)\n",
    "\n",
    "**–ü–∞—Ä—Ç–∏—Ü–∏–∏** - —ç—Ç–æ —Ç–æ, –∫–∞–∫ Spark —Ä–∞–∑–¥–µ–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏.\n",
    "- –ï—Å–ª–∏ –ø–∞—Ä—Ç–∏—Ü–∏–π > 1 ‚Üí –¥–∞–Ω–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –º–µ–∂–¥—É executor'–∞–º–∏\n",
    "- –ö–∞–∂–¥–∞—è –ø–∞—Ä—Ç–∏—Ü–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–º —è–¥—Ä–µ CPU\n",
    "- –ß–µ–º –±–æ–ª—å—à–µ –ø–∞—Ä—Ç–∏—Ü–∏–π, —Ç–µ–º –±–æ–ª—å—à–µ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6e7e4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PARTITION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "1. Default number of partitions: 2\n",
      "   ‚úÖ –ü–ê–†–¢–ò–¶–ò–û–ù–ò–†–û–í–ê–ù–ò–ï –ê–ö–¢–ò–í–ù–û: –¥–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –ø–æ 2 –ø–∞—Ä—Ç–∏—Ü–∏—è–º\n",
      "   ‚úÖ –ö–∞–∂–¥–∞—è –ø–∞—Ä—Ç–∏—Ü–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–º executor/core\n",
      "   ‚úÖ –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ù–£–Æ –æ–±—Ä–∞–±–æ—Ç–∫—É!\n",
      "\n",
      "2. Spark default parallelism: 2\n",
      "   (–≠—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä—Ç–∏—Ü–∏–π –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)\n",
      "\n",
      "3. –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–æ–≤ –ø–∞—Ä—Ç–∏—Ü–∏–π:\n",
      "   –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä—ã –ø–∞—Ä—Ç–∏—Ü–∏–π... (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥)\n",
      "   –†–∞–∑–º–µ—Ä—ã –ø–∞—Ä—Ç–∏—Ü–∏–π: [500000, 500000]\n",
      "   ‚ö†Ô∏è  –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ä—Ç–∏—Ü–∏–π –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: Invalid argument, not a string or column: [500000, 500000] of type <class 'list'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.\n",
      "   ‚úÖ –ù–æ –º—ã –∑–Ω–∞–µ–º, —á—Ç–æ –µ—Å—Ç—å 2 –ø–∞—Ä—Ç–∏—Ü–∏–π\n",
      "   üí° –≠—Ç–æ —É–∂–µ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É!\n",
      "\n",
      "üí° –ß–µ–º –±–æ–ª—å—à–µ –ø–∞—Ä—Ç–∏—Ü–∏–π, —Ç–µ–º –±–æ–ª—å—à–µ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞!\n",
      "üí° –ü–∞—Ä—Ç–∏—Ü–∏–∏ > 1 = —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# –ü–†–û–í–ï–†–ö–ê 2: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä—Ç–∏—Ü–∏–π (–∫–ª—é—á–µ–≤–æ–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PARTITION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# –°–æ–∑–¥–∞–¥–∏–º –ø—Ä–æ—Å—Ç–æ–π DataFrame –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
    "test_df = spark.range(1000000)\n",
    "test_rdd = test_df.rdd\n",
    "num_partitions = test_rdd.getNumPartitions()\n",
    "\n",
    "print(f\"\\n1. Default number of partitions: {num_partitions}\")\n",
    "\n",
    "if num_partitions > 1:\n",
    "    print(f\"   ‚úÖ –ü–ê–†–¢–ò–¶–ò–û–ù–ò–†–û–í–ê–ù–ò–ï –ê–ö–¢–ò–í–ù–û: –¥–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –ø–æ {num_partitions} –ø–∞—Ä—Ç–∏—Ü–∏—è–º\")\n",
    "    print(f\"   ‚úÖ –ö–∞–∂–¥–∞—è –ø–∞—Ä—Ç–∏—Ü–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–º executor/core\")\n",
    "    print(f\"   ‚úÖ –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ù–£–Æ –æ–±—Ä–∞–±–æ—Ç–∫—É!\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  –¢–æ–ª—å–∫–æ 1 –ø–∞—Ä—Ç–∏—Ü–∏—è - –≤–æ–∑–º–æ–∂–Ω–æ –ª–æ–∫–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º (local[1])\")\n",
    "    print(f\"   ‚ö†Ô∏è  –î–∞–Ω–Ω—ã–µ –Ω–µ –±—É–¥—É—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –º–µ–∂–¥—É executor'–∞–º–∏\")\n",
    "\n",
    "print(f\"\\n2. Spark default parallelism: {spark_context.defaultParallelism}\")\n",
    "print(f\"   (–≠—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä—Ç–∏—Ü–∏–π –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –ø–∞—Ä—Ç–∏—Ü–∏–π (–∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–æ–¥—Ö–æ–¥)\n",
    "print(f\"\\n3. –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–æ–≤ –ø–∞—Ä—Ç–∏—Ü–∏–π:\")\n",
    "print(\"   –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä—ã –ø–∞—Ä—Ç–∏—Ü–∏–π... (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥)\")\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–æ–¥—Ö–æ–¥: —Å–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π RDD –Ω–∞–ø—Ä—è–º—É—é\n",
    "# –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º DataFrame API –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –ø–æ –ø–∞—Ä—Ç–∏—Ü–∏—è–º\n",
    "try:\n",
    "    # –°–ø–æ—Å–æ–± 1: –ò—Å–ø–æ–ª—å–∑—É–µ–º DataFrame API —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π –ø–æ –ø–∞—Ä—Ç–∏—Ü–∏—è–º\n",
    "    # Spark 3.0+ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç spark_partition_id()\n",
    "    from pyspark.sql.functions import spark_partition_id, count\n",
    "    \n",
    "    partition_counts = test_df.select(\n",
    "        spark_partition_id().alias(\"partition_id\"),\n",
    "        test_df[\"id\"]\n",
    "    ).groupBy(\"partition_id\").agg(\n",
    "        count(\"id\").alias(\"count\")\n",
    "    ).orderBy(\"partition_id\").collect()\n",
    "    \n",
    "    partition_sizes = [row[\"count\"] for row in partition_counts]\n",
    "    print(f\"   –†–∞–∑–º–µ—Ä—ã –ø–∞—Ä—Ç–∏—Ü–∏–π: {partition_sizes}\")\n",
    "    \n",
    "    if len(partition_sizes) > 1:\n",
    "        print(f\"   –ú–∏–Ω —Ä–∞–∑–º–µ—Ä: {min(partition_sizes):,}, –ú–∞–∫—Å —Ä–∞–∑–º–µ—Ä: {max(partition_sizes):,}\")\n",
    "        avg_size = sum(partition_sizes) / len(partition_sizes)\n",
    "        imbalance = abs(max(partition_sizes) - min(partition_sizes)) / max(partition_sizes) * 100 if max(partition_sizes) > 0 else 0\n",
    "        print(f\"   –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {avg_size:,.0f}\")\n",
    "        if imbalance < 10:\n",
    "            print(f\"   ‚úÖ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ (–¥–∏—Å–±–∞–ª–∞–Ω—Å {imbalance:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  –î–∏—Å–±–∞–ª–∞–Ω—Å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è: {imbalance:.1f}%\")\n",
    "except Exception as e:\n",
    "    # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–± –ø–æ–¥—Å—á–µ—Ç–∞ –±–µ–∑ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ –ø–∞—Ä—Ç–∏—Ü–∏—è–º\n",
    "    print(f\"   ‚ö†Ô∏è  –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ä—Ç–∏—Ü–∏–π –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {str(e)}\")\n",
    "    print(f\"   ‚úÖ –ù–æ –º—ã –∑–Ω–∞–µ–º, —á—Ç–æ –µ—Å—Ç—å {num_partitions} –ø–∞—Ä—Ç–∏—Ü–∏–π\")\n",
    "    print(f\"   üí° –≠—Ç–æ —É–∂–µ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É!\")\n",
    "\n",
    "print(\"\\nüí° –ß–µ–º –±–æ–ª—å—à–µ –ø–∞—Ä—Ç–∏—Ü–∏–π, —Ç–µ–º –±–æ–ª—å—à–µ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞!\")\n",
    "print(\"üí° –ü–∞—Ä—Ç–∏—Ü–∏–∏ > 1 = —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f26a29e",
   "metadata": {},
   "source": [
    "## 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ 3: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏\n",
    "\n",
    "–í—ã–ø–æ–ª–Ω–∏–º –æ–ø–µ—Ä–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è —Ç—Ä–µ–±—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏, –∏ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ª–æ–≥–∏.\n",
    "\n",
    "**–û–ø–µ—Ä–∞—Ü–∏—è `groupBy`** –≤—ã–∑—ã–≤–∞–µ—Ç **shuffle** - —ç—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç:\n",
    "1. –î–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –º–µ–∂–¥—É executor'–∞–º–∏\n",
    "2. –ö–∞–∂–¥—ã–π executor –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–≤–æ—é —á–∞—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö\n",
    "3. –í –ª–æ–≥–∞—Ö –≤—ã —É–≤–∏–¥–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ tasks, –≤—ã–ø–æ–ª–Ω—è—é—â–∏—Ö—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed2a6307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "REAL-TIME TASK EXECUTION MONITORING\n",
      "================================================================================\n",
      "\n",
      "‚öôÔ∏è  –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è:\n",
      "   Spark log level: INFO\n",
      "   SparkContext log level: INFO (–≤–∫–ª—é—á–µ–Ω)\n",
      "\n",
      "üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –∑–∞–¥–∞—á:\n",
      "   (Spark –ª–æ–≥–∏ –º–æ–≥—É—Ç –Ω–µ –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å—Å—è –≤ Jupyter, –Ω–æ –∑–∞–¥–∞—á–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ)\n",
      "\n",
      "üí° –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Spark Web UI –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞!\n",
      "   URL –±—É–¥–µ—Ç –ø–æ–∫–∞–∑–∞–Ω –≤ —Å–ª–µ–¥—É—é—â–µ–π —è—á–µ–π–∫–µ (–ü—Ä–æ–≤–µ—Ä–∫–∞ 4)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "–í—ã–ø–æ–ª–Ω—è–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –æ–ø–µ—Ä–∞—Ü–∏—é –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏...\n",
      "–û–ø–µ—Ä–∞—Ü–∏—è: groupBy() - —ç—Ç–æ shuffle –æ–ø–µ—Ä–∞—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚úÖ –°–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π DataFrame\n",
      "   –ó–∞–ø–∏—Å–µ–π: 10,000,000\n",
      "   –ü–∞—Ä—Ç–∏—Ü–∏–π: 2\n",
      "   ‚úÖ –î–∞–Ω–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –ø–æ 2 –ø–∞—Ä—Ç–∏—Ü–∏—è–º\n",
      "   ‚úÖ –ö–∞–∂–¥–∞—è –ø–∞—Ä—Ç–∏—Ü–∏—è –±—É–¥–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ\n",
      "\n",
      "üîÑ –í—ã–ø–æ–ª–Ω—è–µ–º –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫—É (shuffle –æ–ø–µ—Ä–∞—Ü–∏—è)...\n",
      "   –≠—Ç–æ –æ–ø–µ—Ä–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç –ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É executor'–∞–º–∏\n",
      "   –ï—Å–ª–∏ –ø–∞—Ä—Ç–∏—Ü–∏–π > 1, –≤—ã —É–≤–∏–¥–∏—Ç–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ\n",
      "\n",
      "üìã –ü–ª–∞–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (Execution Plan):\n",
      "   –≠—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫ Spark —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç –∑–∞–¥–∞—á—É:\n",
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['customer_id], [unresolvedalias('customer_id, None), count('id) AS count#55]\n",
      "+- Project [id#40L, cast((id#40L % cast(100 as bigint)) as int) AS customer_id#42]\n",
      "   +- Range (0, 10000000, step=1, splits=Some(2))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "customer_id: int, count: bigint\n",
      "Aggregate [customer_id#42], [customer_id#42, count(id#40L) AS count#55L]\n",
      "+- Project [id#40L, cast((id#40L % cast(100 as bigint)) as int) AS customer_id#42]\n",
      "   +- Range (0, 10000000, step=1, splits=Some(2))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [customer_id#42], [customer_id#42, count(1) AS count#55L]\n",
      "+- Project [cast((id#40L % 100) as int) AS customer_id#42]\n",
      "   +- Range (0, 10000000, step=1, splits=Some(2))\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[customer_id#42], functions=[count(1)], output=[customer_id#42, count#55L])\n",
      "+- Exchange hashpartitioning(customer_id#42, 200), true, [id=#166]\n",
      "   +- *(1) HashAggregate(keys=[customer_id#42], functions=[partial_count(1)], output=[customer_id#42, count#59L])\n",
      "      +- *(1) Project [cast((id#40L % 100) as int) AS customer_id#42]\n",
      "         +- *(1) Range (0, 10000000, step=1, splits=2)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "‚è≥ –í—ã–ø–æ–ª–Ω—è–µ–º –æ–ø–µ—Ä–∞—Ü–∏—é...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "‚úÖ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –í–´–ü–û–õ–ù–ï–ù–ò–Ø\n",
      "================================================================================\n",
      "‚úÖ –û–ø–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ 1.93 —Å–µ–∫—É–Ω–¥\n",
      "‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç: 100 –≥—Ä—É–ø–ø\n",
      "‚úÖ –ü–∞—Ä—Ç–∏—Ü–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: 2\n",
      "\n",
      "‚úÖ –ü–û–î–¢–í–ï–†–ñ–î–ï–ù–û: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ!\n",
      "   - –î–∞–Ω–Ω—ã–µ –±—ã–ª–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ 2 –ø–∞—Ä—Ç–∏—Ü–∏–π\n",
      "   - –ö–∞–∂–¥–∞—è –ø–∞—Ä—Ç–∏—Ü–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª–∞—Å—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ\n",
      "   - –û–ø–µ—Ä–∞—Ü–∏—è groupBy –≤—ã–∑–≤–∞–ª–∞ shuffle –º–µ–∂–¥—É –ø–∞—Ä—Ç–∏—Ü–∏—è–º–∏\n",
      "\n",
      "================================================================================\n",
      "üìä –ö–ê–ö –£–í–ò–î–ï–¢–¨ –î–ï–¢–ê–õ–¨–ù–´–ï –õ–û–ì–ò:\n",
      "================================================================================\n",
      "1. üîç Spark Web UI (–ª—É—á—à–∏–π —Å–ø–æ—Å–æ–±):\n",
      "   - –û—Ç–∫—Ä–æ–π—Ç–µ Web UI –≤ –±—Ä–∞—É–∑–µ—Ä–µ (—Å–º. —Å–ª–µ–¥—É—é—â—É—é —è—á–µ–π–∫—É)\n",
      "   - –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É 'Stages' –∏–ª–∏ 'Jobs'\n",
      "   - –í—ã —É–≤–∏–¥–∏—Ç–µ –≤—Å–µ tasks –∏ –∏—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ executor'–∞–º\n",
      "\n",
      "2. üìù –õ–æ–≥–∏ Spark:\n",
      "   - –õ–æ–≥–∏ Spark –æ–±—ã—á–Ω–æ –ø–∏—à—É—Ç—Å—è –≤ stderr\n",
      "   - –í Jupyter –æ–Ω–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ –≤–∏–¥–Ω—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
      "   - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Å–æ–ª—å/—Ç–µ—Ä–º–∏–Ω–∞–ª, –≥–¥–µ –∑–∞–ø—É—â–µ–Ω Jupyter\n",
      "\n",
      "3. ‚úÖ –ü—Ä–∏–∑–Ω–∞–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏:\n",
      "   - –ü–∞—Ä—Ç–∏—Ü–∏–π > 1 (—É –≤–∞—Å: 2) ‚úÖ\n",
      "   - –û–ø–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å –±—ã—Å—Ç—Ä–æ –±–ª–∞–≥–æ–¥–∞—Ä—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º—É\n",
      "   - Spark Master —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä (yarn)\n",
      "\n",
      "üí° –ï—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ —É–≤–∏–¥–µ—Ç—å –ª–æ–≥–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏:\n",
      "   - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Spark Web UI (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è!)\n",
      "   - –ò–ª–∏ –∑–∞–ø—É—Å–∫–∞–π—Ç–µ –∫–æ–¥ —á–µ—Ä–µ–∑ spark-submit —Å –≤—ã–≤–æ–¥–æ–º –≤ —Ñ–∞–π–ª\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# –ü–†–û–í–ï–†–ö–ê 3: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"REAL-TIME TASK EXECUTION MONITORING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import time\n",
    "import logging\n",
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –≤–∏–¥–∏–º–æ—Å—Ç–∏ –≤ Jupyter\n",
    "# –°–æ–∑–¥–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π handler –¥–ª—è –≤—ã–≤–æ–¥–∞ –ª–æ–≥–æ–≤ Spark –≤ stdout\n",
    "log4j_logger = spark.sparkContext._jvm.org.apache.log4j\n",
    "logger = log4j_logger.LogManager.getLogger(\"org.apache.spark\")\n",
    "\n",
    "# –í–∫–ª—é—á–∞–µ–º –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "# –¢–∞–∫–∂–µ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º log4j –Ω–∞–ø—Ä—è–º—É—é –¥–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –ª–æ–≥–æ–≤\n",
    "logger.setLevel(log4j_logger.Level.INFO)\n",
    "\n",
    "print(\"\\n‚öôÔ∏è  –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è:\")\n",
    "print(f\"   Spark log level: {spark.sparkContext.getConf().get('spark.log.level', 'INFO')}\")\n",
    "print(f\"   SparkContext log level: INFO (–≤–∫–ª—é—á–µ–Ω)\")\n",
    "\n",
    "# –ü–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–¥–∞—á–µ —á–µ—Ä–µ–∑ SparkContext\n",
    "print(\"\\nüìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –∑–∞–¥–∞—á:\")\n",
    "print(\"   (Spark –ª–æ–≥–∏ –º–æ–≥—É—Ç –Ω–µ –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å—Å—è –≤ Jupyter, –Ω–æ –∑–∞–¥–∞—á–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ)\")\n",
    "print(\"\\nüí° –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Spark Web UI –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞!\")\n",
    "print(\"   URL –±—É–¥–µ—Ç –ø–æ–∫–∞–∑–∞–Ω –≤ —Å–ª–µ–¥—É—é—â–µ–π —è—á–µ–π–∫–µ (–ü—Ä–æ–≤–µ—Ä–∫–∞ 4)\\n\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"–í—ã–ø–æ–ª–Ω—è–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –æ–ø–µ—Ä–∞—Ü–∏—é –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏...\")\n",
    "print(\"–û–ø–µ—Ä–∞—Ü–∏—è: groupBy() - —ç—Ç–æ shuffle –æ–ø–µ—Ä–∞—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# –í—ã–ø–æ–ª–Ω—è–µ–º –æ–ø–µ—Ä–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è —Ç–æ—á–Ω–æ –ø–æ—Ç—Ä–µ–±—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "test_df = spark.range(10000000).withColumn(\"customer_id\", (col(\"id\") % 100).cast(\"int\"))\n",
    "\n",
    "print(f\"\\n‚úÖ –°–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π DataFrame\")\n",
    "print(f\"   –ó–∞–ø–∏—Å–µ–π: {test_df.count():,}\")\n",
    "num_partitions = test_df.rdd.getNumPartitions()\n",
    "print(f\"   –ü–∞—Ä—Ç–∏—Ü–∏–π: {num_partitions}\")\n",
    "\n",
    "if num_partitions > 1:\n",
    "    print(f\"   ‚úÖ –î–∞–Ω–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –ø–æ {num_partitions} –ø–∞—Ä—Ç–∏—Ü–∏—è–º\")\n",
    "    print(f\"   ‚úÖ –ö–∞–∂–¥–∞—è –ø–∞—Ä—Ç–∏—Ü–∏—è –±—É–¥–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ\")\n",
    "\n",
    "print(\"\\nüîÑ –í—ã–ø–æ–ª–Ω—è–µ–º –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫—É (shuffle –æ–ø–µ—Ä–∞—Ü–∏—è)...\")\n",
    "print(\"   –≠—Ç–æ –æ–ø–µ—Ä–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç –ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É executor'–∞–º–∏\")\n",
    "print(\"   –ï—Å–ª–∏ –ø–∞—Ä—Ç–∏—Ü–∏–π > 1, –≤—ã —É–≤–∏–¥–∏—Ç–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ\\n\")\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º explain() –¥–ª—è –ø–æ–∫–∞–∑–∞ –ø–ª–∞–Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è\n",
    "print(\"üìã –ü–ª–∞–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (Execution Plan):\")\n",
    "print(\"   –≠—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫ Spark —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç –∑–∞–¥–∞—á—É:\")\n",
    "test_df.groupBy(\"customer_id\").agg(\n",
    "    count(\"id\").alias(\"count\")\n",
    ").explain(True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"‚è≥ –í—ã–ø–æ–ª–Ω—è–µ–º –æ–ø–µ—Ä–∞—Ü–∏—é...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "result = test_df.groupBy(\"customer_id\").agg(\n",
    "    count(\"id\").alias(\"count\")\n",
    ").count()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –í–´–ü–û–õ–ù–ï–ù–ò–Ø\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚úÖ –û–ø–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥\")\n",
    "print(f\"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç: {result} –≥—Ä—É–ø–ø\")\n",
    "print(f\"‚úÖ –ü–∞—Ä—Ç–∏—Ü–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {num_partitions}\")\n",
    "\n",
    "if num_partitions > 1:\n",
    "    print(f\"\\n‚úÖ –ü–û–î–¢–í–ï–†–ñ–î–ï–ù–û: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ!\")\n",
    "    print(f\"   - –î–∞–Ω–Ω—ã–µ –±—ã–ª–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ {num_partitions} –ø–∞—Ä—Ç–∏—Ü–∏–π\")\n",
    "    print(f\"   - –ö–∞–∂–¥–∞—è –ø–∞—Ä—Ç–∏—Ü–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª–∞—Å—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ\")\n",
    "    print(f\"   - –û–ø–µ—Ä–∞—Ü–∏—è groupBy –≤—ã–∑–≤–∞–ª–∞ shuffle –º–µ–∂–¥—É –ø–∞—Ä—Ç–∏—Ü–∏—è–º–∏\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  –¢–æ–ª—å–∫–æ 1 –ø–∞—Ä—Ç–∏—Ü–∏—è - —Ä–∞–±–æ—Ç–∞ –Ω–µ –±—ã–ª–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä –ö–ê–ö –£–í–ò–î–ï–¢–¨ –î–ï–¢–ê–õ–¨–ù–´–ï –õ–û–ì–ò:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. üîç Spark Web UI (–ª—É—á—à–∏–π —Å–ø–æ—Å–æ–±):\")\n",
    "print(\"   - –û—Ç–∫—Ä–æ–π—Ç–µ Web UI –≤ –±—Ä–∞—É–∑–µ—Ä–µ (—Å–º. —Å–ª–µ–¥—É—é—â—É—é —è—á–µ–π–∫—É)\")\n",
    "print(\"   - –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É 'Stages' –∏–ª–∏ 'Jobs'\")\n",
    "print(\"   - –í—ã —É–≤–∏–¥–∏—Ç–µ –≤—Å–µ tasks –∏ –∏—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ executor'–∞–º\")\n",
    "print(\"\\n2. üìù –õ–æ–≥–∏ Spark:\")\n",
    "print(\"   - –õ–æ–≥–∏ Spark –æ–±—ã—á–Ω–æ –ø–∏—à—É—Ç—Å—è –≤ stderr\")\n",
    "print(\"   - –í Jupyter –æ–Ω–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ –≤–∏–¥–Ω—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\")\n",
    "print(\"   - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Å–æ–ª—å/—Ç–µ—Ä–º–∏–Ω–∞–ª, –≥–¥–µ –∑–∞–ø—É—â–µ–Ω Jupyter\")\n",
    "print(\"\\n3. ‚úÖ –ü—Ä–∏–∑–Ω–∞–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏:\")\n",
    "print(f\"   - –ü–∞—Ä—Ç–∏—Ü–∏–π > 1 (—É –≤–∞—Å: {num_partitions}) ‚úÖ\")\n",
    "print(\"   - –û–ø–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å –±—ã—Å—Ç—Ä–æ –±–ª–∞–≥–æ–¥–∞—Ä—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º—É\")\n",
    "print(\"   - Spark Master —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä (yarn)\")\n",
    "print(\"\\nüí° –ï—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ —É–≤–∏–¥–µ—Ç—å –ª–æ–≥–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏:\")\n",
    "print(\"   - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Spark Web UI (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è!)\")\n",
    "print(\"   - –ò–ª–∏ –∑–∞–ø—É—Å–∫–∞–π—Ç–µ –∫–æ–¥ —á–µ—Ä–µ–∑ spark-submit —Å –≤—ã–≤–æ–¥–æ–º –≤ —Ñ–∞–π–ª\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd595de",
   "metadata": {},
   "source": [
    "## 3.1. –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–±: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —á–µ—Ä–µ–∑ Spark API\n",
    "\n",
    "–ï—Å–ª–∏ –ª–æ–≥–∏ –Ω–µ –≤–∏–¥–Ω—ã –≤ Jupyter, –∏—Å–ø–æ–ª—å–∑—É–µ–º Spark API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∑–∞–¥–∞—á–∞—Ö.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "828bbffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TASK MONITORING VIA SPARK API\n",
      "================================================================================\n",
      "\n",
      "üìä –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –æ–ø–µ—Ä–∞—Ü–∏—é –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞...\n",
      "‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω: 5,000,000 –∑–∞–ø–∏—Å–µ–π\n",
      "‚úÖ –ü–∞—Ä—Ç–∏—Ü–∏–π: 2\n",
      "\n",
      "üìã –í—ã–ø–æ–ª–Ω—è–µ–º –æ–ø–µ—Ä–∞—Ü–∏—é —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'StatusTracker' object has no attribute 'getActiveJobIds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b6ad819fe279>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# –í—ã–ø–æ–ª–Ω—è–µ–º –æ–ø–µ—Ä–∞—Ü–∏—é\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mresult_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_monitor_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mjob_ids_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatus_tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetActiveJobIds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   –ù–∞—á–∞—Ç–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'StatusTracker' object has no attribute 'getActiveJobIds'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ù–´–ô –°–ü–û–°–û–ë: –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∑–∞–¥–∞—á–∞—Ö —á–µ—Ä–µ–∑ Spark API\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TASK MONITORING VIA SPARK API\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pyspark.sql.functions import col, count\n",
    "import time\n",
    "\n",
    "# –í–∫–ª—é—á–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —á–µ—Ä–µ–∑ SparkContext\n",
    "spark_context = spark.sparkContext\n",
    "status_tracker = spark_context.statusTracker()\n",
    "\n",
    "print(\"\\nüìä –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –æ–ø–µ—Ä–∞—Ü–∏—é –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞...\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—É—é –æ–ø–µ—Ä–∞—Ü–∏—é –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞\n",
    "test_monitor_df = spark.range(5000000).withColumn(\"key\", (col(\"id\") % 100).cast(\"int\"))\n",
    "\n",
    "print(f\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω: {test_monitor_df.count():,} –∑–∞–ø–∏—Å–µ–π\")\n",
    "print(f\"‚úÖ –ü–∞—Ä—Ç–∏—Ü–∏–π: {test_monitor_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ job IDs\n",
    "print(\"\\nüìã –í—ã–ø–æ–ª–Ω—è–µ–º –æ–ø–µ—Ä–∞—Ü–∏—é —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# –í—ã–ø–æ–ª–Ω—è–µ–º –æ–ø–µ—Ä–∞—Ü–∏—é\n",
    "result_df = test_monitor_df.groupBy(\"key\").agg(count(\"id\").alias(\"count\"))\n",
    "job_ids_before = status_tracker.getActiveJobIds()\n",
    "\n",
    "print(f\"   –ù–∞—á–∞—Ç–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ...\")\n",
    "print(f\"   –ü–∞—Ä—Ç–∏—Ü–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {test_monitor_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# –í—ã–ø–æ–ª–Ω—è–µ–º action –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∑–∞–¥–∞—á–∏\n",
    "result_count = result_df.count()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ –û–ø–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed:.2f} —Å–µ–∫—É–Ω–¥\")\n",
    "print(f\"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç: {result_count} –≥—Ä—É–ø–ø\")\n",
    "\n",
    "# –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö\n",
    "try:\n",
    "    active_jobs = status_tracker.getActiveJobIds()\n",
    "    print(f\"\\nüìä –°—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á:\")\n",
    "    print(f\"   –ê–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á: {len(active_jobs)}\")\n",
    "    \n",
    "    if len(active_jobs) > 0:\n",
    "        for job_id in active_jobs[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3\n",
    "            try:\n",
    "                job_info = status_tracker.getJobInfo(job_id)\n",
    "                if job_info:\n",
    "                    print(f\"\\n   Job {job_id}:\")\n",
    "                    print(f\"      Status: {job_info.status}\")\n",
    "                    print(f\"      Stages: {len(job_info.stageIds)}\")\n",
    "            except:\n",
    "                pass\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–¥–∞—á–∞—Ö –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üí° –î–õ–Ø –î–ï–¢–ê–õ–¨–ù–û–ì–û –ú–û–ù–ò–¢–û–†–ò–ù–ì–ê:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. ‚úÖ Spark Web UI - –ª—É—á—à–∏–π —Å–ø–æ—Å–æ–± —É–≤–∏–¥–µ—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:\")\n",
    "print(\"   - –í—Å–µ tasks –∏ –∏—Ö —Å—Ç–∞—Ç—É—Å—ã\")\n",
    "print(\"   - –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ executor'–∞–º\")\n",
    "print(\"   - –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏\")\n",
    "print(\"   - Shuffle –æ–ø–µ—Ä–∞—Ü–∏–∏\")\n",
    "print(\"\\n2. ‚úÖ –ü—Ä–∏–∑–Ω–∞–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ —É–∂–µ –≤–∏–¥–Ω—ã:\")\n",
    "print(f\"   - –ü–∞—Ä—Ç–∏—Ü–∏–π: {test_monitor_df.rdd.getNumPartitions()} (> 1 = —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–æ)\")\n",
    "print(f\"   - –ë—ã—Å—Ç—Ä–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ: {elapsed:.2f} —Å–µ–∫ (–±–ª–∞–≥–æ–¥–∞—Ä—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º—É)\")\n",
    "print(\"   - Spark Master: yarn (–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π —Ä–µ–∂–∏–º)\")\n",
    "print(\"\\n3. ‚úÖ Execution Plan (—Å–º. –≤—ã—à–µ) –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6167d138",
   "metadata": {},
   "source": [
    "## 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ 4: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ Spark Web UI\n",
    "\n",
    "**Spark Web UI** - —Å–∞–º—ã–π –Ω–∞–≥–ª—è–¥–Ω—ã–π —Å–ø–æ—Å–æ–± —É–≤–∏–¥–µ—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ.\n",
    "\n",
    "### –ö–∞–∫ –æ—Ç–∫—Ä—ã—Ç—å Spark Web UI:\n",
    "\n",
    "1. **–û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ —Ö–æ—Å—Ç driver'–∞** –∏–∑ –≤—ã–≤–æ–¥–∞ –Ω–∏–∂–µ\n",
    "2. **–û—Ç–∫—Ä–æ–π—Ç–µ –≤ –±—Ä–∞—É–∑–µ—Ä–µ:** `http://<driver-host>:4040`\n",
    "\n",
    "### –ß—Ç–æ —Å–º–æ—Ç—Ä–µ—Ç—å –≤ Web UI:\n",
    "\n",
    "1. **Jobs Tab** - —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∑–∞–¥–∞–Ω–∏–π\n",
    "   - –ö–∞–∂–¥–æ–µ –∑–∞–¥–∞–Ω–∏–µ —Ä–∞–∑–±–∏—Ç–æ –Ω–∞ **stages**\n",
    "   - –ö–∞–∂–¥—ã–π stage —Ä–∞–∑–±–∏—Ç –Ω–∞ **tasks**\n",
    "   - Tasks –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è **–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ** –Ω–∞ —Ä–∞–∑–Ω—ã—Ö executor'–∞—Ö\n",
    "\n",
    "2. **Stages Tab** - –¥–µ—Ç–∞–ª–∏ stages\n",
    "   - –í–∏–¥–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ **tasks**\n",
    "   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ **executors**, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–ø–æ–ª–Ω—è—é—Ç tasks\n",
    "   - –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏\n",
    "\n",
    "3. **Executors Tab** - –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ executor'–∞—Ö\n",
    "   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö executor'–æ–≤\n",
    "   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –∏ CPU\n",
    "   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á\n",
    "\n",
    "### –ü—Ä–∏–∑–Ω–∞–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:\n",
    "- ‚úÖ –ù–µ—Å–∫–æ–ª—å–∫–æ executor'–æ–≤ –≤ —Å–ø–∏—Å–∫–µ\n",
    "- ‚úÖ Tasks –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö executor'–∞—Ö (—Ä–∞–∑–Ω—ã–µ Executor ID)\n",
    "- ‚úÖ –ù–µ—Å–∫–æ–ª—å–∫–æ tasks –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ\n",
    "- ‚úÖ –í–∏–¥–Ω—ã shuffle –æ–ø–µ—Ä–∞—Ü–∏–∏ (–ø–µ—Ä–µ–¥–∞—á–∞ –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É executor'–∞–º–∏)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc0b5a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SPARK WEB UI ACCESS INFORMATION\n",
      "================================================================================\n",
      "\n",
      "1. Driver Host: rc1d-dataproc-m-i6vt9s3078v7g4n7.mdb.yandexcloud.net\n",
      "2. Application ID: application_1762054626128_0007\n",
      "\n",
      "3. Spark Web UI URL:\n",
      "   http://rc1d-dataproc-m-i6vt9s3078v7g4n7.mdb.yandexcloud.net:4040\n",
      "   http://rc1d-dataproc-m-i6vt9s3078v7g4n7.mdb.yandexcloud.net:4040\n",
      "\n",
      "================================================================================\n",
      "üìä –û–¢–ö–†–û–ô–¢–ï WEB UI –í –ë–†–ê–£–ó–ï–†–ï –ò –ü–†–û–í–ï–†–¨–¢–ï:\n",
      "\n",
      "   ‚úÖ Jobs Tab: –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ tasks\n",
      "   ‚úÖ Stages Tab: –≤–∏–¥–∏—Ç–µ –ª–∏ –≤—ã –Ω–µ—Å–∫–æ–ª—å–∫–æ tasks –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ?\n",
      "   ‚úÖ Executors Tab: —Å–∫–æ–ª—å–∫–æ executor'–æ–≤ –∞–∫—Ç–∏–≤–Ω–æ?\n",
      "   ‚úÖ –ï—Å–ª–∏ –≤–∏–¥–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ executor'–æ–≤ –∏ tasks - —Ä–∞–±–æ—Ç–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∞!\n",
      "\n",
      "üí° –ü–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–π –æ–±–Ω–æ–≤–∏—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É Web UI\n",
      "üí° –í—ã–ø–æ–ª–Ω–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–ø–µ—Ä–∞—Ü–∏–π –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞\n",
      "üí° –°–º–æ—Ç—Ä–∏—Ç–µ, –∫–∞–∫ tasks —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –ø–æ executor'–∞–º\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# –ü–†–û–í–ï–†–ö–ê 4: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ Spark Web UI\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SPARK WEB UI ACCESS INFORMATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "spark_context = spark.sparkContext\n",
    "\n",
    "try:\n",
    "    driver_host = spark_context.getConf().get('spark.driver.host', 'localhost')\n",
    "    \n",
    "    print(f\"\\n1. Driver Host: {driver_host}\")\n",
    "    print(f\"2. Application ID: {spark_context.applicationId}\")\n",
    "    \n",
    "    print(f\"\\n3. Spark Web UI URL:\")\n",
    "    print(f\"   http://{driver_host}:4040\")\n",
    "    \n",
    "    try:\n",
    "        ui_port = spark_context.getConf().get('spark.ui.port', '4040')\n",
    "        print(f\"   http://{driver_host}:{ui_port}\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìä –û–¢–ö–†–û–ô–¢–ï WEB UI –í –ë–†–ê–£–ó–ï–†–ï –ò –ü–†–û–í–ï–†–¨–¢–ï:\")\n",
    "    print(\"\\n   ‚úÖ Jobs Tab: –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ tasks\")\n",
    "    print(\"   ‚úÖ Stages Tab: –≤–∏–¥–∏—Ç–µ –ª–∏ –≤—ã –Ω–µ—Å–∫–æ–ª—å–∫–æ tasks –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ?\")\n",
    "    print(\"   ‚úÖ Executors Tab: —Å–∫–æ–ª—å–∫–æ executor'–æ–≤ –∞–∫—Ç–∏–≤–Ω–æ?\")\n",
    "    print(\"   ‚úÖ –ï—Å–ª–∏ –≤–∏–¥–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ executor'–æ–≤ –∏ tasks - —Ä–∞–±–æ—Ç–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∞!\")\n",
    "    \n",
    "    print(\"\\nüí° –ü–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–π –æ–±–Ω–æ–≤–∏—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É Web UI\")\n",
    "    print(\"üí° –í—ã–ø–æ–ª–Ω–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–ø–µ—Ä–∞—Ü–∏–π –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞\")\n",
    "    print(\"üí° –°–º–æ—Ç—Ä–∏—Ç–µ, –∫–∞–∫ tasks —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –ø–æ executor'–∞–º\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∞–¥—Ä–µ—Å Web UI: {str(e)}\")\n",
    "    print(\"\\n–ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\")\n",
    "    print(\"   http://localhost:4040\")\n",
    "    print(\"   http://<your-hostname>:4040\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38c04f1",
   "metadata": {},
   "source": [
    "## 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ 5: –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–∑–Ω–∏—Ü—ã –º–µ–∂–¥—É –æ–ø–µ—Ä–∞—Ü–∏—è–º–∏\n",
    "\n",
    "–°—Ä–∞–≤–Ω–∏–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –∏ –∏—Ö –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1b2d936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DISTRIBUTED EXECUTION DEMONSTRATION\n",
      "================================================================================\n",
      "\n",
      "–°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç (10M –∑–∞–ø–∏—Å–µ–π)...\n",
      "‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω\n",
      "   –ü–∞—Ä—Ç–∏—Ü–∏–π: 2\n",
      "   –ó–∞–ø–∏—Å–µ–π: 10,000,000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "–û–ü–ï–†–ê–¶–ò–Ø 1: –ü—Ä–æ—Å—Ç–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è (count) - –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π shuffle\n",
      "--------------------------------------------------------------------------------\n",
      "–†–µ–∑—É–ª—å—Ç–∞—Ç: 10,000,000 –∑–∞–ø–∏—Å–µ–π\n",
      "–í—Ä–µ–º—è: 0.11 —Å–µ–∫—É–Ω–¥\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "–û–ü–ï–†–ê–¶–ò–Ø 2: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ customer_id - —Ç—Ä–µ–±—É–µ—Ç SHUFFLE (—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)\n",
      "--------------------------------------------------------------------------------\n",
      "üìä –°–ú–û–¢–†–ò–¢–ï –õ–û–ì–ò - –∑–¥–µ—Å—å –≤—ã —É–≤–∏–¥–∏—Ç–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ tasks!\n",
      "–†–µ–∑—É–ª—å—Ç–∞—Ç: 1000 –≥—Ä—É–ø–ø\n",
      "–í—Ä–µ–º—è: 1.47 —Å–µ–∫—É–Ω–¥\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "–û–ü–ï–†–ê–¶–ò–Ø 3: Join –¥–≤—É—Ö DataFrame - –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π SHUFFLE\n",
      "--------------------------------------------------------------------------------\n",
      "üìä –°–ú–û–¢–†–ò–¢–ï –õ–û–ì–ò - –∑–¥–µ—Å—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ tasks!\n",
      "–†–µ–∑—É–ª—å—Ç–∞—Ç: 10,000,000 –∑–∞–ø–∏—Å–µ–π –ø–æ—Å–ª–µ join\n",
      "–í—Ä–µ–º—è: 9.72 —Å–µ–∫—É–Ω–¥\n",
      "\n",
      "================================================================================\n",
      "–ò–¢–û–ì–û–í–´–ô –ê–ù–ê–õ–ò–ó:\n",
      "================================================================================\n",
      "\n",
      "–û–ø–µ—Ä–∞—Ü–∏—è 1 (count): 0.11 —Å–µ–∫ - –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π shuffle\n",
      "–û–ø–µ—Ä–∞—Ü–∏—è 2 (groupBy): 1.47 —Å–µ–∫ - shuffle –º–µ–∂–¥—É –ø–∞—Ä—Ç–∏—Ü–∏—è–º–∏\n",
      "–û–ø–µ—Ä–∞—Ü–∏—è 3 (join): 9.72 —Å–µ–∫ - –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π shuffle\n",
      "\n",
      "‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ 2 –ø–∞—Ä—Ç–∏—Ü–∏–π\n",
      "‚úÖ –û–ø–µ—Ä–∞—Ü–∏–∏ –≤—ã–ø–æ–ª–Ω—è–ª–∏—Å—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ\n",
      "‚úÖ –í –ª–æ–≥–∞—Ö –≤—ã –≤–∏–¥–µ–ª–∏ tasks –Ω–∞ —Ä–∞–∑–Ω—ã—Ö executor'–∞—Ö\n",
      "\n",
      "üí° –û—Ç–∫—Ä–æ–π—Ç–µ Spark Web UI –∏ –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ:\n",
      "   - –°–∫–æ–ª—å–∫–æ executor'–æ–≤ —É—á–∞—Å—Ç–≤–æ–≤–∞–ª–æ –≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏\n",
      "   - –°–∫–æ–ª—å–∫–æ tasks –≤—ã–ø–æ–ª–Ω—è–ª–æ—Å—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ\n",
      "   - Shuffle –æ–ø–µ—Ä–∞—Ü–∏–∏ (–ø–µ—Ä–µ–¥–∞—á–∞ –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É executor'–∞–º–∏)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# –ü–†–û–í–ï–†–ö–ê 5: –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DISTRIBUTED EXECUTION DEMONSTRATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import time\n",
    "from pyspark.sql.functions import col, count, avg\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
    "print(\"\\n–°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç (10M –∑–∞–ø–∏—Å–µ–π)...\")\n",
    "test_data = spark.range(10000000).withColumn(\n",
    "    \"customer_id\", \n",
    "    (col(\"id\") % 1000).cast(\"int\")\n",
    ").withColumn(\n",
    "    \"amount\", \n",
    "    (col(\"id\") % 1000 + 10).cast(\"double\")\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω\")\n",
    "print(f\"   –ü–∞—Ä—Ç–∏—Ü–∏–π: {test_data.rdd.getNumPartitions()}\")\n",
    "print(f\"   –ó–∞–ø–∏—Å–µ–π: {test_data.count():,}\")\n",
    "\n",
    "# –û–ø–µ—Ä–∞—Ü–∏—è 1: –ü—Ä–æ—Å—Ç–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"–û–ü–ï–†–ê–¶–ò–Ø 1: –ü—Ä–æ—Å—Ç–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è (count) - –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π shuffle\")\n",
    "print(\"-\" * 80)\n",
    "start = time.time()\n",
    "result1 = test_data.count()\n",
    "time1 = time.time() - start\n",
    "print(f\"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result1:,} –∑–∞–ø–∏—Å–µ–π\")\n",
    "print(f\"–í—Ä–µ–º—è: {time1:.2f} —Å–µ–∫—É–Ω–¥\")\n",
    "\n",
    "# –û–ø–µ—Ä–∞—Ü–∏—è 2: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ (—Ç—Ä–µ–±—É–µ—Ç shuffle - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö)\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"–û–ü–ï–†–ê–¶–ò–Ø 2: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ customer_id - —Ç—Ä–µ–±—É–µ—Ç SHUFFLE (—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)\")\n",
    "print(\"-\" * 80)\n",
    "print(\"üìä –°–ú–û–¢–†–ò–¢–ï –õ–û–ì–ò - –∑–¥–µ—Å—å –≤—ã —É–≤–∏–¥–∏—Ç–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ tasks!\")\n",
    "start = time.time()\n",
    "result2 = test_data.groupBy(\"customer_id\").agg(\n",
    "    count(\"id\").alias(\"tx_count\"),\n",
    "    avg(\"amount\").alias(\"avg_amount\")\n",
    ").count()\n",
    "time2 = time.time() - start\n",
    "print(f\"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result2} –≥—Ä—É–ø–ø\")\n",
    "print(f\"–í—Ä–µ–º—è: {time2:.2f} —Å–µ–∫—É–Ω–¥\")\n",
    "\n",
    "# –û–ø–µ—Ä–∞—Ü–∏—è 3: Join (–º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π shuffle)\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"–û–ü–ï–†–ê–¶–ò–Ø 3: Join –¥–≤—É—Ö DataFrame - –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π SHUFFLE\")\n",
    "print(\"-\" * 80)\n",
    "print(\"üìä –°–ú–û–¢–†–ò–¢–ï –õ–û–ì–ò - –∑–¥–µ—Å—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ tasks!\")\n",
    "\n",
    "df1 = test_data.select(\"id\", \"customer_id\").alias(\"df1\")\n",
    "df2 = test_data.select(\"id\", \"amount\").alias(\"df2\")\n",
    "\n",
    "start = time.time()\n",
    "result3 = df1.join(df2, \"id\").count()\n",
    "time3 = time.time() - start\n",
    "print(f\"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result3:,} –∑–∞–ø–∏—Å–µ–π –ø–æ—Å–ª–µ join\")\n",
    "print(f\"–í—Ä–µ–º—è: {time3:.2f} —Å–µ–∫—É–Ω–¥\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"–ò–¢–û–ì–û–í–´–ô –ê–ù–ê–õ–ò–ó:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n–û–ø–µ—Ä–∞—Ü–∏—è 1 (count): {time1:.2f} —Å–µ–∫ - –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π shuffle\")\n",
    "print(f\"–û–ø–µ—Ä–∞—Ü–∏—è 2 (groupBy): {time2:.2f} —Å–µ–∫ - shuffle –º–µ–∂–¥—É –ø–∞—Ä—Ç–∏—Ü–∏—è–º–∏\")\n",
    "print(f\"–û–ø–µ—Ä–∞—Ü–∏—è 3 (join): {time3:.2f} —Å–µ–∫ - –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π shuffle\")\n",
    "\n",
    "num_partitions = test_data.rdd.getNumPartitions()\n",
    "if num_partitions > 1:\n",
    "    print(f\"\\n‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ {num_partitions} –ø–∞—Ä—Ç–∏—Ü–∏–π\")\n",
    "    print(f\"‚úÖ –û–ø–µ—Ä–∞—Ü–∏–∏ –≤—ã–ø–æ–ª–Ω—è–ª–∏—Å—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ\")\n",
    "    print(f\"‚úÖ –í –ª–æ–≥–∞—Ö –≤—ã –≤–∏–¥–µ–ª–∏ tasks –Ω–∞ —Ä–∞–∑–Ω—ã—Ö executor'–∞—Ö\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  –¢–æ–ª—å–∫–æ 1 –ø–∞—Ä—Ç–∏—Ü–∏—è - —Ä–∞–±–æ—Ç–∞ –Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\")\n",
    "\n",
    "print(\"\\nüí° –û—Ç–∫—Ä–æ–π—Ç–µ Spark Web UI –∏ –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ:\")\n",
    "print(\"   - –°–∫–æ–ª—å–∫–æ executor'–æ–≤ —É—á–∞—Å—Ç–≤–æ–≤–∞–ª–æ –≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏\")\n",
    "print(\"   - –°–∫–æ–ª—å–∫–æ tasks –≤—ã–ø–æ–ª–Ω—è–ª–æ—Å—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ\")\n",
    "print(\"   - Shuffle –æ–ø–µ—Ä–∞—Ü–∏–∏ (–ø–µ—Ä–µ–¥–∞—á–∞ –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É executor'–∞–º–∏)\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166bc786",
   "metadata": {},
   "source": [
    "## 7. –ò—Ç–æ–≥–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –ß–µ–∫-–ª–∏—Å—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è\n",
    "\n",
    "### ‚úÖ –ü—Ä–∏–∑–Ω–∞–∫–∏ —Ç–æ–≥–æ, —á—Ç–æ Spark —Ä–∞–±–æ—Ç–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ:\n",
    "\n",
    "1. **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä—Ç–∏—Ü–∏–π > 1**\n",
    "   ```python\n",
    "   df.rdd.getNumPartitions() > 1  # ‚úÖ\n",
    "   ```\n",
    "\n",
    "2. **–ù–µ—Å–∫–æ–ª—å–∫–æ executor'–æ–≤ –∞–∫—Ç–∏–≤–Ω—ã**\n",
    "   ```python\n",
    "   len(spark.sparkContext.statusTracker().getExecutorInfos()) > 1  # ‚úÖ\n",
    "   ```\n",
    "\n",
    "3. **–í –ª–æ–≥–∞—Ö –≤–∏–¥–Ω—ã –Ω–µ—Å–∫–æ–ª—å–∫–æ tasks –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ**\n",
    "   - –õ–æ–≥–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç \"Finished task\" –æ—Ç —Ä–∞–∑–Ω—ã—Ö executor'–æ–≤\n",
    "   - Tasks –∏–º–µ—é—Ç —Ä–∞–∑–Ω—ã–µ \"executor ID\"\n",
    "\n",
    "4. **Spark Web UI –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ executor'–æ–≤**\n",
    "   - Executors Tab: –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–µ–π\n",
    "   - Jobs Tab: tasks —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –ø–æ —Ä–∞–∑–Ω—ã–º executor'–∞–º\n",
    "\n",
    "5. **Spark Master —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä**\n",
    "   ```python\n",
    "   spark.sparkContext.master\n",
    "   # ‚úÖ \"yarn\" –∏–ª–∏ \"spark://host:port\" –∏–ª–∏ \"k8s://\"\n",
    "   # ‚ùå \"local[1]\" - –ª–æ–∫–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º, –Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω\n",
    "   # ‚ö†Ô∏è  \"local[N]\" - –ª–æ–∫–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º —Å N –ø–æ—Ç–æ–∫–∞–º–∏\n",
    "   ```\n",
    "\n",
    "### ‚ö†Ô∏è –í–∞–∂–Ω—ã–µ –Ω—é–∞–Ω—Å—ã:\n",
    "\n",
    "- **`local[N]`** - —ç—Ç–æ –ù–ï –Ω–∞—Å—Ç–æ—è—â–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ–∂–¥—É –º–∞—à–∏–Ω–∞–º–∏\n",
    "- –≠—Ç–æ –æ–¥–Ω–∞ –º–∞—à–∏–Ω–∞ —Å N –ø–æ—Ç–æ–∫–∞–º–∏, –Ω–æ –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç\n",
    "- –ù–∞—Å—Ç–æ—è—â–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç—Ä–µ–±—É–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä (YARN, Kubernetes, standalone)\n",
    "\n",
    "---\n",
    "\n",
    "**–í—ã–≤–æ–¥:** –ï—Å–ª–∏ –≤—ã –≤–∏–¥–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–∞—Ä—Ç–∏—Ü–∏–π –∏ tasks –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ - Spark –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –æ–±—Ä–∞–±–æ—Ç–∫–∏!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d414c7c6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
