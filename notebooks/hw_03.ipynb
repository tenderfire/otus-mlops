{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51ae4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b87d3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .appName(\"OTUS\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "        .config(\"spark.executor.memory\", \"2g\")\n",
    "        .config(\"spark.driver.memory\", \"1g\")\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53c3aee7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'Python 3'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Unable to get resolved server information for ms-toolsai.jupyter:_builtin.jupyterServerUrlProvider:b82edd12-82c1-45e1-85a4-6d4a9d3b8ced"
     ]
    }
   ],
   "source": [
    "# Используем системную команду head для просмотра файла на HDFS\n",
    "!hdfs dfs -cat data/2022-11-04.txt | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ee349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Файл имеет заголовок с разделителем '|', но данные с разделителем ','\n",
    "# Опеределяем схему\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# Вариант 1: Читаем datetime как строку, потом конвертируем\n",
    "schema = StructType([\n",
    "    StructField(\"tranaction_id\", LongType(), True),\n",
    "    StructField(\"tx_datetime\", StringType(), True),  # сначала как строка\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"terminal_id\", IntegerType(), True),\n",
    "    StructField(\"tx_amount\", DoubleType(), True),\n",
    "    StructField(\"tx_time_seconds\", LongType(), True),\n",
    "    StructField(\"tx_time_days\", IntegerType(), True),\n",
    "    StructField(\"tx_fraud\", IntegerType(), True),\n",
    "    StructField(\"tx_fraud_scenario\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Читаем файл и конвертируем datetime в правильный тип\n",
    "df = (\n",
    "    spark.read.csv(\n",
    "        \"data/2022-11-04.txt\",\n",
    "        sep=\",\",           # разделитель - запятая\n",
    "        schema=schema      # используем определённую схему\n",
    "    )\n",
    "    .filter(col(\"tranaction_id\").isNotNull())  # убираем строку с заголовком\n",
    "    .withColumn(\"tx_datetime\", to_timestamp(\"tx_datetime\", \"yyyy-MM-dd HH:mm:ss\"))  # конвертируем в timestamp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c519ea7",
   "metadata": {},
   "source": [
    "Проверим схему данных с правильным типом datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56e9618f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1e4846eda7dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Проверим типы данных\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Посмотрим на данные\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Проверим типы данных\n",
    "df.printSchema()\n",
    "\n",
    "# Посмотрим на данные\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5df84a",
   "metadata": {},
   "source": [
    "### Преимущества использования TimestampType:\n",
    "1. **Меньше места** в parquet (эффективное хранение)\n",
    "2. **Быстрые операции** с датами (фильтрация, сортировка, группировка)\n",
    "3. **Встроенные функции** для работы с датами (year, month, day, hour и т.д.)\n",
    "4. **Автоматическая валидация** формата даты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85622519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример работы с datetime колонкой\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute, dayofweek\n",
    "\n",
    "# Извлечём различные компоненты даты\n",
    "df.select(\n",
    "    \"tx_datetime\",\n",
    "    year(\"tx_datetime\").alias(\"year\"),\n",
    "    month(\"tx_datetime\").alias(\"month\"),\n",
    "    dayofmonth(\"tx_datetime\").alias(\"day\"),\n",
    "    hour(\"tx_datetime\").alias(\"hour\"),\n",
    "    minute(\"tx_datetime\").alias(\"minute\"),\n",
    "    dayofweek(\"tx_datetime\").alias(\"day_of_week\")  # 1 = Воскресенье, 7 = Суббота\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a0c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраним в parquet\n",
    "(\n",
    "    riiid_df\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(\"data/2022-11-04.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa84477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим содержимое директории parquet\n",
    "!hdfs dfs -ls data/2022-11-04.parquet\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Размер файлов внутри parquet директории:\")\n",
    "print(\"=\"*50)\n",
    "!hdfs dfs -du -h data/2022-11-04.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ba70b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество записей в parquet: 46998983\n",
      "Схема данных:\n",
      "root\n",
      " |-- tranaction_id: long (nullable = true)\n",
      " |-- tx_datetime: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- terminal_id: integer (nullable = true)\n",
      " |-- tx_amount: double (nullable = true)\n",
      " |-- tx_time_seconds: long (nullable = true)\n",
      " |-- tx_time_days: integer (nullable = true)\n",
      " |-- tx_fraud: integer (nullable = true)\n",
      " |-- tx_fraud_scenario: integer (nullable = true)\n",
      "\n",
      "\n",
      "Первые 5 записей:\n",
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|tranaction_id|        tx_datetime|customer_id|terminal_id|tx_amount|tx_time_seconds|tx_time_days|tx_fraud|tx_fraud_scenario|\n",
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|   1838826044|2022-11-07 15:08:53|     850577|        324|    65.72|      101401733|        1173|       0|                0|\n",
      "|   1838826045|2022-11-07 15:09:45|     850579|        734|    83.88|      101401785|        1173|       0|                0|\n",
      "|   1838826046|2022-11-07 12:29:14|     850579|        533|    87.64|      101392154|        1173|       0|                0|\n",
      "|   1838826047|2022-11-07 06:50:02|     850580|        826|     3.81|      101371802|        1173|       0|                0|\n",
      "|   1838826048|2022-11-07 11:08:58|     850581|        115|    50.42|      101387338|        1173|       1|                2|\n",
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|tranaction_id|        tx_datetime|customer_id|terminal_id|tx_amount|tx_time_seconds|tx_time_days|tx_fraud|tx_fraud_scenario|\n",
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|   1838826044|2022-11-07 15:08:53|     850577|        324|    65.72|      101401733|        1173|       0|                0|\n",
      "|   1838826045|2022-11-07 15:09:45|     850579|        734|    83.88|      101401785|        1173|       0|                0|\n",
      "|   1838826046|2022-11-07 12:29:14|     850579|        533|    87.64|      101392154|        1173|       0|                0|\n",
      "|   1838826047|2022-11-07 06:50:02|     850580|        826|     3.81|      101371802|        1173|       0|                0|\n",
      "|   1838826048|2022-11-07 11:08:58|     850581|        115|    50.42|      101387338|        1173|       1|                2|\n",
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Читаем parquet обратно\n",
    "df_from_parquet = spark.read.parquet(\"data/2022-11-04.parquet\")\n",
    "\n",
    "print(f\"Количество записей в parquet: {df_from_parquet.count()}\")\n",
    "print(f\"Схема данных:\")\n",
    "df_from_parquet.printSchema()\n",
    "print(\"\\nПервые 5 записей:\")\n",
    "df_from_parquet.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
